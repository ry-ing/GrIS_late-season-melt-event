{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel-1 image pair velocity flux gate discharge calculations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import warnings\n",
    "import xarray as xr\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import dask\n",
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from osgeo import gdal\n",
    "import subprocess\n",
    "import sys\n",
    "import math\n",
    "import shapely\n",
    "\n",
    "import geopandas\n",
    "import rioxarray as rio\n",
    "from rioxarray.merge import merge_datasets\n",
    "from shapely.geometry import mapping\n",
    "from rasterio.enums import Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_data_dir =  #internal data dir within the github repo\n",
    "data_dir = # External data (e.g. bedmachine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all functions used for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_raster(raster): # displays useful information on rasters\n",
    "    print(\n",
    "        f\"shape: {raster.rio.shape}\\n\"\n",
    "        f\"resolution: {raster.rio.resolution()}\\n\"\n",
    "        f\"bounds: {raster.rio.bounds()}\\n\"\n",
    "        f\"CRS: {raster.rio.crs}\\n\"\n",
    "    )\n",
    "\n",
    "def retrieve_timestamp(tiff_list): # retrieves timestamp from .tif filename and then adds it as a dimension\n",
    "    time = []\n",
    "    for f, n in zip(tiff_list, range(len(tiff_list))):\n",
    "        match = re.search(r\"_((\\d+)_(\\d+))_250m\", f) # search through filename and retreive pair start/end time\n",
    "        pair_start = pd.to_datetime(match.group(2), format='%Y%m%d')\n",
    "        pair_end = pd.to_datetime(match.group(3), format='%Y%m%d')\n",
    "        time_between = pair_start + (pair_end - pair_start)/2 # calculate time between\n",
    "        time.append(time_between)\n",
    "    return time\n",
    "\n",
    "\n",
    "def ROI_select(DATA, ROI_dir, invert=None): # clips raster to given shapefile area, option to invert or not\n",
    "        #print('Clipping DATA to ROI')\n",
    "        shapefile_dir = ROI_dir\n",
    "        glacier_shape = geopandas.read_file(shapefile_dir)\n",
    "        DATA.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=True)\n",
    "        DATA.rio.write_crs(\"epsg:3413\", inplace=True)\n",
    "        DATA = DATA.rio.clip(glacier_shape.geometry.apply(mapping), glacier_shape.crs, drop=True, all_touched=True, invert=invert)\n",
    "        return DATA\n",
    "\n",
    "def common_grid(data, name, raster=None): # reprojects rio datasets to a common grid system\n",
    "    if raster == True:\n",
    "        data = data.to_dataset('band') # convert to dataset\n",
    "        data = data.rename({1: name}) # rename band to speed\n",
    "    data.rio.write_crs(\"epsg:3413\", inplace=True) # set CRS, north polar stereographic\n",
    "\n",
    "    if name=='EC':\n",
    "        commongrid = data.rio.reproject_match(geotiffs_S_avg, resampling=Resampling.nearest) # resample to velocity grid\n",
    "    else:\n",
    "        commongrid = data.rio.reproject_match(geotiffs_S_avg, resampling=Resampling.bilinear) # resample to velocity grid\n",
    "\n",
    "    commongrid = commongrid.assign_coords({\"x\": geotiffs_S_avg.x, \"y\": geotiffs_S_avg.y,}) # set to velocity grid, due to tiny differences\n",
    "    commongrid = ROI_select(commongrid, internal_data_dir + '\\data\\discharge\\GIMP_SWG_landmask.shp', invert=True) # clip to ice\n",
    "    \n",
    "    return commongrid\n",
    "\n",
    "def retrieve_office_std(flat_u, flat_v, u_std_t, v_std_t):\n",
    "        mean_u = np.nanmean(flat_u)\n",
    "        mean_v = np.nanmean(flat_v)\n",
    "        mean_s = (mean_u**2 + mean_v**2)**0.5\n",
    "\n",
    "        error = (u_std_t*(((mean_u**2)**0.5)/mean_s)) + (v_std_t*(((mean_v**2)**0.5)/mean_s))\n",
    "\n",
    "        return error\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flux-gate discharge function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_list_U = glob.glob(data_dir + '/AS_S1_velocities/SWG/U/U_*timefiltered.tif') # U tifs\n",
    "tiff_list_V = glob.glob(data_dir + '/AS_S1_velocities/SWG/V/V_*timefiltered.tif') # V tifs\n",
    "gimp_dem_list = glob.glob(data_dir + '/gimpdem*.tif') # surface elevation DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create variable used for time axis\n",
    "time_var = xr.Variable('time', retrieve_timestamp(tiff_list_U)) # Assuming U AND V have the same timestamps\n",
    "\n",
    "#Average_velocity FOR COMMON GRID\n",
    "geotiffs_S_avg = xr.open_rasterio(data_dir + '/AS_S1_velocities/SWG/S/EXTRA/S_avg_20220102_20230531_250m_timefiltered.tif', chunks='auto')\n",
    "geotiffs_S_avg = geotiffs_S_avg.to_dataset('band') # convert to dataset\n",
    "geotiffs_S_avg = geotiffs_S_avg.rename({1: 'S'}) # rename band to speed\n",
    "\n",
    "# # U component of velocity\n",
    "geotiffs_U = xr.concat([xr.open_rasterio(i, chunks='auto') for i in tiff_list_U], dim=time_var)\n",
    "geotiffs_U = geotiffs_U.to_dataset('band') # convert to dataset\n",
    "geotiffs_U = geotiffs_U.rename({1: 'U'}) # rename band to speed\n",
    "\n",
    "# # V component of velocity\n",
    "geotiffs_V = xr.concat([xr.open_rasterio(i, chunks='auto') for i in tiff_list_V], dim=time_var)\n",
    "geotiffs_V = geotiffs_V.to_dataset('band') # convert to dataset\n",
    "geotiffs_V = geotiffs_V.rename({1: 'V'}) # rename band to speed\n",
    "\n",
    "geotiffs_U.rio.write_crs(\"epsg:3413\", inplace=True) # set CRS, north polar stereographic\n",
    "geotiffs_V.rio.write_crs(\"epsg:3413\", inplace=True) # set CRS, north polar stereographic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yang et al. 2022, surface elevation change dataset\n",
    "This dataset provides the long-term elevation change rate data of the GrIS in three different periods using the ICESat data (from February 2003 to October 2009), Cryosat-2 data (from August 2010 to October 2018) and ICESat-2 data (from October 2018 to December 2020). The dataset is named based on the data. It contains three raster files and three shapefiles. The raster files are the interpolated 5 km Ã— 5 km elevation change rates of the GrIS using ordinary kriging in the Stereographic North Pole projection coordinate system (EPSG 3413)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BedMachineV5 = xr.open_dataset(data_dir + '/BedMachineGreenland-v5.nc', engine='netcdf4')\n",
    "BedMachineV5['x'] = BedMachineV5['x'].astype('float64')\n",
    "BedMachineV5['y'] = BedMachineV5['y'].astype('float64')\n",
    "\n",
    "BedMachineV5_commongrid = common_grid(BedMachineV5['bed'], 'bed', raster=False)\n",
    "\n",
    "BedMachineV5_error = common_grid(BedMachineV5['errbed'], 'errbed', raster=False)\n",
    "BedMachineV5_thickness = common_grid(BedMachineV5['thickness'], 'thickness', raster=False)\n",
    "\n",
    "# Load in surface elevation data\n",
    "gimp_dem_commongrid = common_grid(BedMachineV5['surface'], 'elev', raster=False)\n",
    "\n",
    "gimp_dem_maxdate = xr.open_rasterio(data_dir + '/gimpdem_merged_SW_maxdate.tif') \n",
    "gimp_dem_maxdate_commongrid = common_grid(gimp_dem_maxdate, 'date', raster=True)\n",
    "\n",
    "gimp_dem_mindate = xr.open_rasterio(data_dir + '/gimpdem_merged_SW_mindate.tif') \n",
    "gimp_dem_mindate_commongrid = common_grid(gimp_dem_mindate, 'date', raster=True)\n",
    "\n",
    "cryosat2 = xr.open_rasterio(data_dir + '/elevation_change_dataset_GrIS/Cryosat2_EC.tif')\n",
    "cryosat2.rio.write_nodata(-3.40282e+38, inplace=True) # set nodata value\n",
    "cryosat2_commongrid = common_grid(cryosat2, 'EC', raster=True) # 2010 TO 2018\n",
    "\n",
    "ICEsat2 = xr.open_rasterio(data_dir + '/elevation_change_dataset_GrIS/ICEsat2_EC.tif')\n",
    "ICEsat2.rio.write_nodata(-3.40282e+38, inplace=True) # set nodata value\n",
    "ICEsat2_commongrid = common_grid(ICEsat2, 'EC', raster=True) # 2018 TO 2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Setting all fill/NaN values to zero elevation change rate #####\n",
    "cryosat2_commongrid_2 = cryosat2_commongrid.where(cryosat2_commongrid['EC']<10000, 0)\n",
    "cryosat2_commongrid_3 = cryosat2_commongrid_2.where(cryosat2_commongrid_2['EC']>-10000, 0)\n",
    "ICEsat2_commongrid_2 = ICEsat2_commongrid.where(ICEsat2_commongrid['EC']<10000, 0)\n",
    "ICEsat2_commongrid_3 = ICEsat2_commongrid_2.where(ICEsat2_commongrid_2['EC']>-10000, 0)\n",
    "\n",
    "ICEsat2_commongrid_3.to_netcdf(data_dir + '/elevation_change_dataset_GrIS/ICEsat2_EC_3.nc')\n",
    "\n",
    "year_list = pd.date_range(start='2016', end='2024', freq='1AS') # list of years in velocithy dataset to act as timestamps\n",
    "\n",
    "initial_elev = gimp_dem_commongrid.values#['surface'].values\n",
    "for i, year in enumerate(year_list):\n",
    "    if year <= pd.to_datetime('2017'): # USE GIMPDEM + CRYOSAT2\n",
    "        mult_factor = int((pd.to_datetime('2020') - year)/np.timedelta64(365, 'D')) # calculate how many times to apply cryosat2 elevation change (m/year)\n",
    "        surf_elev = initial_elev + (-1 * cryosat2_commongrid_3['EC'].values * mult_factor) # x -1 to reverse cryosat2 elevation change\n",
    "\n",
    "        surf_elev_dataset = xr.Dataset(data_vars=dict(surf_elev=([\"y\", \"x\"], surf_elev)), \n",
    "                                coords=dict(x=([\"x\"], gimp_dem_commongrid.x.values),y=([\"y\"], gimp_dem_commongrid.y.values)))\n",
    "\n",
    "        surf_elev_dataset = surf_elev_dataset.assign_coords(time = year)\n",
    "        surf_elev_dataset = surf_elev_dataset.expand_dims(dim=\"time\")\n",
    "\n",
    "        if i == 0:\n",
    "            surf_elev_dataset_complete = surf_elev_dataset\n",
    "        else:\n",
    "            surf_elev_dataset_complete = xr.concat([surf_elev_dataset_complete, surf_elev_dataset], dim='time')\n",
    "        \n",
    "    elif (pd.to_datetime('2018') <= year < pd.to_datetime('2020')): # USE GIMPDEM + ICEsat2:\n",
    "        mult_factor = int((pd.to_datetime('2020') - year)/np.timedelta64(365, 'D')) # calculate how many times to apply cryosat2 elevation change (m/year)\n",
    "        surf_elev = initial_elev + (-1 * ICEsat2_commongrid_3['EC'].values  * mult_factor) # x -1 to reverse cryosat2 elevation change\n",
    "        \n",
    "        surf_elev_dataset = xr.Dataset(data_vars=dict(surf_elev=([\"y\", \"x\"], surf_elev)), \n",
    "                        coords=dict(x=([\"x\"], gimp_dem_commongrid.x.values),y=([\"y\"], gimp_dem_commongrid.y.values)))\n",
    "\n",
    "        surf_elev_dataset = surf_elev_dataset.assign_coords(time = year)\n",
    "        surf_elev_dataset = surf_elev_dataset.expand_dims(dim=\"time\")\n",
    "\n",
    "        surf_elev_dataset_complete = xr.concat([surf_elev_dataset_complete, surf_elev_dataset], dim='time')\n",
    "    \n",
    "    elif year==pd.to_datetime('2020'): # USE GIMP DEM\n",
    "        surf_elev = initial_elev\n",
    "   \n",
    "        surf_elev_dataset = xr.Dataset(data_vars=dict(surf_elev=([\"y\", \"x\"], surf_elev)), \n",
    "                        coords=dict(x=([\"x\"], gimp_dem_commongrid.x.values),y=([\"y\"], gimp_dem_commongrid.y.values)))\n",
    "\n",
    "        surf_elev_dataset = surf_elev_dataset.assign_coords(time = year)\n",
    "        surf_elev_dataset = surf_elev_dataset.expand_dims(dim=\"time\")\n",
    "\n",
    "        surf_elev_dataset_complete = xr.concat([surf_elev_dataset_complete, surf_elev_dataset], dim='time')\n",
    "    \n",
    "    elif year > pd.to_datetime('2020'): # USE GIMPDEM + ICEsat2\n",
    "        mult_factor = int((year - pd.to_datetime('2020'))/np.timedelta64(365, 'D')) # calculate how many times to apply cryosat2 elevation change (m/year)\n",
    "        surf_elev = initial_elev + (ICEsat2_commongrid_3['EC'].values  * mult_factor) \n",
    "        \n",
    "        surf_elev_dataset = xr.Dataset(data_vars=dict(surf_elev=([\"y\", \"x\"], surf_elev)), \n",
    "                        coords=dict(x=([\"x\"], gimp_dem_commongrid.x.values),y=([\"y\"], gimp_dem_commongrid.y.values)))\n",
    "\n",
    "        surf_elev_dataset = surf_elev_dataset.assign_coords(time = year)\n",
    "        surf_elev_dataset = surf_elev_dataset.expand_dims(dim=\"time\")\n",
    "\n",
    "        surf_elev_dataset_complete = xr.concat([surf_elev_dataset_complete, surf_elev_dataset], dim='time')\n",
    "\n",
    "for i, year in enumerate(year_list):\n",
    "    if i ==0:\n",
    "        ice_thickess = surf_elev_dataset_complete['surf_elev'].loc[dict(time=year)].values - BedMachineV5_commongrid.values \n",
    "\n",
    "        ice_thickness_dataset_complete = xr.Dataset(data_vars=dict(ice_thickness=([\"y\", \"x\"], ice_thickess)), \n",
    "                        coords=dict(x=([\"x\"], gimp_dem_commongrid.x.values),y=([\"y\"], gimp_dem_commongrid.y.values)))\n",
    "\n",
    "        ice_thickness_dataset_complete = ice_thickness_dataset_complete.assign_coords(time = year)\n",
    "        ice_thickness_dataset_complete = ice_thickness_dataset_complete.expand_dims(dim=\"time\")\n",
    "    else:\n",
    "        ice_thickess = surf_elev_dataset_complete['surf_elev'].loc[dict(time=year)].values - BedMachineV5_commongrid.values\n",
    "\n",
    "        ice_thickness_dataset = xr.Dataset(data_vars=dict(ice_thickness=([\"y\", \"x\"], ice_thickess)), \n",
    "                        coords=dict(x=([\"x\"], gimp_dem_commongrid.x.values),y=([\"y\"], gimp_dem_commongrid.y.values)))\n",
    "\n",
    "        ice_thickness_dataset = ice_thickness_dataset.assign_coords(time = year)\n",
    "        ice_thickness_dataset = ice_thickness_dataset.expand_dims(dim=\"time\")\n",
    "\n",
    "        ice_thickness_dataset_complete = xr.concat([ice_thickness_dataset_complete, ice_thickness_dataset], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gate = 'KNS_gate'\n",
    "# gate_shapefile = glob.glob(internal_data_dir + '/data/discharge/flux_gates/%s.shp' % gate)\n",
    "\n",
    "# test = ROI_select(ice_thickness_dataset_complete, gate_shapefile[0], invert=False)\n",
    "# test = test.where(test['ice_thickness']<10000000000000, np.nan)\n",
    "# #test = test.where(test['elev']>0, np.nan)\n",
    "\n",
    "# bedmachine_test = ROI_select(BedMachineV5_thickness, gate_shapefile[0], invert=False)\n",
    "# bedmachine_test = bedmachine_test.where(test['ice_thickness']<10000000000000, np.nan)\n",
    "# print('BED MACHINE:', bedmachine_test.mean().values)\n",
    "# print('\\n')\n",
    "\n",
    "# for i in test.time:\n",
    "#     print(test['ice_thickness'].sel(time=i).mean().values)\n",
    "#     # plt.imshow(test.sel(time=i)['surf_elev'])\n",
    "#     # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# LOAD IN METADATA FOR STANDARD DEVIATION #########################\n",
    "col_names = ['start_year', 'start_month', 'start_day', 'end_year', 'end_month', 'end_day', 'min_off-ice_vel', 'max_off-ice_vel', 'mean_off-ice_vel', 'median_off-ice_vel', 'std_off-ice_vel']\n",
    "\n",
    "u_std_file = pd.read_csv(data_dir + '/AS_S1_velocities/SWG/U/U_metadata_250m_20220102_20230531.txt', header=None, delimiter=',', usecols=[0,1,2,6,7,8,12,13,14,15,16], names=col_names) #skips columns with 0\n",
    "u_std_file['start_date'] = pd.to_datetime(dict(year=u_std_file.start_year, month=u_std_file.start_month, day=u_std_file.start_day))\n",
    "u_std_file['end_date'] = pd.to_datetime(dict(year=u_std_file.end_year, month=u_std_file.end_month, day=u_std_file.end_day))\n",
    "u_std_file = u_std_file.drop(columns=['start_year', 'start_month', 'start_day', 'end_year', 'end_month', 'end_day'])\n",
    "u_std_file['time_between'] = u_std_file['start_date'] + (u_std_file['end_date'] - u_std_file['start_date'])/2 # calculate time between\n",
    "u_std_file.index = u_std_file['time_between']\n",
    "\n",
    "v_std_file = pd.read_csv(data_dir + '/AS_S1_velocities/SWG/V/V_metadata_250m_20220102_20230531.txt', header=None, delimiter=',', usecols=[0,1,2,6,7,8,12,13,14,15,16], names=col_names) #skips columns with 0\n",
    "v_std_file['start_date'] = pd.to_datetime(dict(year=v_std_file.start_year, month=v_std_file.start_month, day=v_std_file.start_day))\n",
    "v_std_file['end_date'] = pd.to_datetime(dict(year=v_std_file.end_year, month=v_std_file.end_month, day=v_std_file.end_day))\n",
    "v_std_file = v_std_file.drop(columns=['start_year', 'start_month', 'start_day', 'end_year', 'end_month', 'end_day'])\n",
    "v_std_file['time_between'] = v_std_file['start_date'] + (v_std_file['end_date'] - v_std_file['start_date'])/2 # calculate time between\n",
    "v_std_file.index = v_std_file['time_between']\n",
    "\n",
    "u_std = u_std_file['std_off-ice_vel']\n",
    "v_std = v_std_file['std_off-ice_vel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BedMachineV5_thickness = xr.Dataset(data_vars=dict(ice_thickness=([\"y\", \"x\"], BedMachineV5_thickness.values)), \n",
    "                        coords=dict(x=([\"x\"], gimp_dem_commongrid.x.values),y=([\"y\"], gimp_dem_commongrid.y.values)))\n",
    "\n",
    "\n",
    "BedMachineV5_thickness = BedMachineV5_thickness.where(BedMachineV5_thickness < 10000, np.nan)\n",
    "BedMachineV5_thickness['ice_thickness'].rio.write_crs(\"epsg:3413\", inplace=True) # set CRS, north polar stereographic\n",
    "BedMachineV5_thickness['ice_thickness'].rio.write_nodata(np.nan)\n",
    "\n",
    "###############################\n",
    "ice_thickness_dataset_complete['ice_thickness'].rio.write_crs(\"epsg:3413\", inplace=True) # set CRS, north polar stereographic\n",
    "ice_thickness_dataset_complete['ice_thickness'].rio.write_nodata(0)\n",
    "\n",
    "ice_thickness_dataset_complete = ice_thickness_dataset_complete.where(ice_thickness_dataset_complete < 10000, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROI_select(DATA, ROI_dir, invert=None):\n",
    "        ''' Clips Xarray DataArray to a given shapefile'''\n",
    "        shapefile_dir = ROI_dir \n",
    "        glacier_shape = geopandas.read_file(shapefile_dir)\n",
    "\n",
    "        DATA.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=True)\n",
    "        DATA.rio.write_crs(\"epsg:3413\", inplace=True)\n",
    "        DATA = DATA.rio.clip(glacier_shape.geometry.apply(mapping), glacier_shape.crs, drop=True, all_touched=True, invert=invert)\n",
    "\n",
    "        return DATA\n",
    "\n",
    "def perp_vel_v2(U, V, azimuth_gate_angles):\n",
    "    ''' calculates perpendicualr velocity to gate for a given pixel '''\n",
    "    azimuth = azimuth_gate_angles\n",
    "    magnitude_vel = math.sqrt(U**2 + V**2) # velocity magnitude\n",
    "    b = math.degrees(math.atan2(U,V)) # bearing of u and v velocity in DEGREES\n",
    "\n",
    "    ang_diff = math.radians(azimuth+90) - math.radians(b)  #azimuth of gate is 90 degrees away from velocity perpendicular to gate!\n",
    "\n",
    "    v_perp = magnitude_vel * math.cos(math.radians(ang_diff))\n",
    "    print('Original velocity: %.2f, Perp velocity: %.2f \\nb: %.2f, angle difference: %.2f' % (magnitude_vel, v_perp, b, ang_diff))\n",
    "    return v_perp\n",
    "\n",
    "def retrieve_gate_azimuth(gate_shapefile):\n",
    "    ''' Given a shapefile of a gate, calculates the azimuth of each gate segment and the coords of the centre of the segment'''\n",
    "    glacier_shape = geopandas.read_file(gate_shapefile)\n",
    "    a = glacier_shape.iloc[0].geometry\n",
    "    coords = [c for c in a.coords]\n",
    "    segments = [shapely.geometry.LineString([a, b]) for a, b in zip(coords,coords[1:])]\n",
    "\n",
    "    azimuth_list = [] # list of azimuth angles for each segment\n",
    "    azimuth_coord_list = [] # list of coods of each centroid\n",
    "\n",
    "    for seg in range(len(segments)): # loops through each segments along the gate\n",
    "            p1 = segments[seg].coords[0] # point 1 of segments\n",
    "            p2 = segments[seg].coords[1] # point 2 of segment\n",
    "            midpoint = segments[seg].centroid # coordinates of the middle of the segment\n",
    "            coord_mid = (midpoint.x, midpoint.y)\n",
    "\n",
    "            angle =  (-math.degrees(math.atan2(p2[1]-p1[1], p2[0]-p1[0]))+90) # calculating the azimuth angle         \n",
    "            azimuth_coord_list.append(coord_mid)\n",
    "            azimuth_list.append(angle)\n",
    "\n",
    "    return azimuth_list, azimuth_coord_list\n",
    "\n",
    "def flatten_gate(var): # flattens xarray/rio dataset into 1d array, used in flux-gate discharge calculations\n",
    "    flatten = var.values.flatten()\n",
    "    flatten_no_nan = flatten[~np.isnan(flatten)] # gets rid of NaN values\n",
    "        \n",
    "    return flatten_no_nan\n",
    "\n",
    "def flux_gate_discharge(U, V, ice_thickness, gate, internal_data_dir):\n",
    "    ice_density = 917 # kg m-3\n",
    "    w = 250 # width of velocity pixel\n",
    "    velocity_depth_reduction = 1 # to reduce velocity with depth\n",
    "\n",
    "    gate_shapefile = glob.glob(internal_data_dir + '/data/discharge/flux_gates/%s.shp' % gate)\n",
    "    print(gate_shapefile)\n",
    "    print(internal_data_dir + '/data/discharge/flux_gates/%s.shp' % gate)\n",
    "\n",
    "    # clip variables to pre-defined gate ROI\n",
    "    U_gate = ROI_select(U['U'], gate_shapefile[0], invert=False)\n",
    "    V_gate = ROI_select(V['V'], gate_shapefile[0], invert=False)\n",
    "\n",
    "    bedmachine_thickness_gate = ROI_select(BedMachineV5_thickness, gate_shapefile[0], invert=False)\n",
    "    thickness_gate = ROI_select(ice_thickness_dataset_complete, gate_shapefile[0], invert=False)\n",
    "    bed_error = ROI_select(BedMachineV5_error, gate_shapefile[0]) # bedmachine error\n",
    "\n",
    "    time =  pd.to_datetime(U_gate['time'].values)\n",
    "    azimuth_list, azimuth_coord_list = retrieve_gate_azimuth(gate_shapefile[0])\n",
    "\n",
    "    U_gate_xy = U_gate[0] # get rid of time dimension\n",
    "    U_gate_xy_stacked = U_gate_xy.stack(xy=['x','y']) # stacks x and y coords \n",
    "    U_gate_xy_stacked = U_gate_xy_stacked[U_gate_xy_stacked.notnull()] # gets rid of NaN values\n",
    "    coord_tuples = U_gate_xy_stacked.xy.values\n",
    "\n",
    "    #### Loop below finds coordinate in U and V closest to the middle of each gate segment, uses this to select azimuth angle\n",
    "    closest_distance = float('inf')\n",
    "    azimuth_gate_angles = []\n",
    "    for x1,y1 in coord_tuples:\n",
    "         for index, (x2, y2) in enumerate(azimuth_coord_list):\n",
    "              distance = math.sqrt((x1-x2)**2 + (y1-y2)**2) #\n",
    "              if distance < closest_distance:\n",
    "                    index_closest = index\n",
    "              azimuth_gate_angles.append(azimuth_list[index_closest])\n",
    "\n",
    "    if not (time == pd.to_datetime(V_gate['time'].values)).all():\n",
    "        print(\"U and V time do not match!\") # Check if the U file and V file are the same timestep\n",
    "        sys.exit()\n",
    "\n",
    "    discharge_time_array = [] # defining arrays to append data too\n",
    "    gate_discharge_array = []\n",
    "    u_error_array = [] # upper error bound\n",
    "    l_error_array = [] # lower error bound\n",
    "    width_averaged_thickness_array = []\n",
    "    width_averaged_thickness_bedmachine_array = []\n",
    "\n",
    "    for t, time_val in zip(range(len(time)), time):\n",
    "        flat_U = flatten_gate(U_gate.isel(time=t)) # flatten U and V componenets so we can perform calculations with them\n",
    "        flat_V = flatten_gate(V_gate.isel(time=t))\n",
    "        bed_error_t = bed_error.mean().values\n",
    "\n",
    "        u_std_t = u_std[time_val.strftime('%Y-%m-%d')]\n",
    "        v_std_t = v_std[time_val.strftime('%Y-%m-%d')]\n",
    "        vel_error_t = retrieve_office_std(flat_U, flat_V, u_std_t, v_std_t)\n",
    "\n",
    "        if not any(V == -9999 for V in flat_V) and not any(U == -9999 for U in flat_U): # and not any(U > 0 for U in flat_U) and np.isnan(flat_U).any()==False and np.isnan(flat_V).any()==False:     \n",
    "            U_gate_t_df = U_gate.isel(time=t) # for retrieving time\n",
    "            U_gate_t = flat_U\n",
    "            V_gate_t = flat_V\n",
    "\n",
    "            time_thickness = pd.to_datetime(U_gate_t_df['time'].values).strftime('%Y') # creating time coord for the time-varying thickness estimate\n",
    "            thickness_gate_year = thickness_gate['ice_thickness'].loc[dict(time=pd.to_datetime(time_thickness))] # select ice thickness at specified timestep\n",
    "\n",
    "            S = [perp_vel_v2(U,V,a) for U,V,a in zip(U_gate_t, V_gate_t, azimuth_gate_angles)] # calculate velocity perpendicular to gate using already defined function \n",
    "            \n",
    "            discharge = S * flatten_gate(thickness_gate_year) * w * ice_density * velocity_depth_reduction   #calculate density at each bin\n",
    "            gate_discharge_sum = np.sum(discharge) / 1e12 # sum along gate and convert to Gt year-1\n",
    "\n",
    "            dis_error_max = (S+vel_error_t) * flatten_gate((thickness_gate_year+bed_error_t)) * w * ice_density  # upper bound with error and k\n",
    "            gate_error_max = np.sum(dis_error_max) / 1e12\n",
    "\n",
    "            dis_error_min = 0.8*(S-vel_error_t) * flatten_gate((thickness_gate_year-bed_error_t)) * w * ice_density # lower band with error and k\n",
    "            gate_error_min= np.sum(dis_error_min) / 1e12\n",
    "\n",
    "            u_error = gate_error_max - gate_discharge_sum\n",
    "            l_error = gate_discharge_sum - gate_error_min\n",
    "\n",
    "            width_averaged_thickness = np.mean(flatten_gate(thickness_gate_year))\n",
    "            width_averaged_thickness_bedmachine = np.mean(flatten_gate(bedmachine_thickness_gate['ice_thickness']))\n",
    "\n",
    "            time_t = pd.to_datetime(U_gate_t_df['time'].values)\n",
    "            discharge_time_array.append(time_t)\n",
    "            gate_discharge_array.append(gate_discharge_sum)\n",
    "            u_error_array.append(u_error)\n",
    "            l_error_array.append(l_error)\n",
    "            width_averaged_thickness_array.append(width_averaged_thickness)\n",
    "            width_averaged_thickness_bedmachine_array.append(width_averaged_thickness_bedmachine)\n",
    "            print('Time:', time_t, ', Discharge:', gate_discharge_sum, 'Gt yr-1')\n",
    "        else:\n",
    "             print('SKIPPED', pd.to_datetime(U_gate.isel(time=t)['time'].values))\n",
    "            \n",
    "    df_array = np.array([discharge_time_array, gate_discharge_array, u_error_array, l_error_array, width_averaged_thickness_array, width_averaged_thickness_bedmachine_array])\n",
    "    df_array = np.transpose(df_array)\n",
    "    discharge_df = pd.DataFrame(df_array, columns = ['time', 'discharge', 'u_error', 'l_error', 'width_averaged_thickness', 'width_averaged_thickness_bedmachine'])\n",
    "    discharge_df.to_csv(internal_data_dir + '/data/discharge/%s_ice_discharge.csv' % gate)\n",
    "    print('Created discharge rates for gate: %s' % gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_list =['AS_gate', 'KNS_gate']\n",
    "for gate in gate_list:\n",
    "    print('\\n\\n\\n---------------------------------------------')\n",
    "    print('Calculating flux gate for:', gate, '\\n')\n",
    "    flux_gate_discharge(geotiffs_U, geotiffs_V, ice_thickness_dataset_complete, gate, internal_data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JIF_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6b27a61ade767387bf933b3ed50815f8a3916e37851216800ac10e527c50735"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
