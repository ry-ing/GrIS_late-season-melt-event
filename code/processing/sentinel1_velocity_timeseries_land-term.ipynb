{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "import geopandas\n",
    "import rioxarray\n",
    "from shapely.geometry import mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_timestamp(tiff_list): # function to get timestamp from filename for inputting into an xarray dataframe\n",
    "    print('\\nRetrieving timestamp\\n')\n",
    "    time = []\n",
    "    repeat_time_list = []\n",
    "    for f, n in zip(tiff_list, range(len(tiff_list))):\n",
    "        match = re.search(r\"S_((\\d+)_(\\d+))_250m\", f) # search through filename and retreive pair start/end time\n",
    "        pair_start = pd.to_datetime(match.group(2), format='%Y%m%d')\n",
    "        pair_end = pd.to_datetime(match.group(3), format='%Y%m%d')\n",
    "        time_between = pair_start + (pair_end - pair_start)/2 # calculate time between\n",
    "        \n",
    "        repeat_time = pair_end - pair_start\n",
    "        repeat_time = float(repeat_time.days)\n",
    "\n",
    "        time.append(time_between)\n",
    "        repeat_time_list.append(repeat_time)\n",
    "    return time, repeat_time_list\n",
    "\n",
    "def ROI_select(DATA, ROI_dir, invert=None):\n",
    "        #print('Clipping DATA to ROI')\n",
    "        shapefile_dir = ROI_dir\n",
    "        glacier_shape = geopandas.read_file(shapefile_dir)\n",
    "        DATA.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=True)\n",
    "        DATA.rio.write_crs(\"epsg:3413\", inplace=True) \n",
    "        DATA = DATA.rio.clip(glacier_shape.geometry.apply(mapping), glacier_shape.crs, drop=True, all_touched=True)\n",
    "        return DATA\n",
    "\n",
    "def calculate_STD(data): # error is the standard deviation of all off-ice pixels. \n",
    "    print('\\nCalculating STD for velocities\\n')\n",
    "    land_data = ROI_select(data, internal_data_dir + '\\velocity\\ROI_shapefiles\\GIMP_land_mask.shp')\n",
    "    land_data = land_data.where(land_data['S'] !=-9999) # drop -9999 values\n",
    "    land_data_std = land_data['S'].std(dim=['y', 'x'], skipna=True).values # compute standard deviation of land pixels\n",
    "\n",
    "    data['STD'] = (('time'), land_data_std) # adding standard deviation to dataframe\n",
    "    data['STD'].attrs['long_name'] = 'standard_deviation'\n",
    "    return data\n",
    "\n",
    "def extract_ROI_velocity(S_data, U_data, V_data, ROI, U_STD, V_STD):\n",
    "    print('Loading and extracting ROI....')\n",
    "    ROI_ice_velocity = ROI_select(S_data, internal_data_dir + '\\velocity\\ROI_shapefiles\\%s.shp' % ROI)\n",
    "    U_ROI_ice_velocity = ROI_select(U_data, internal_data_dir + '\\velocity\\ROI_shapefiles\\%s.shp' % ROI)\n",
    "    V_ROI_ice_velocity = ROI_select(V_data, internal_data_dir + '\\velocity\\ROI_shapefiles\\%s.shp' % ROI)\n",
    "\n",
    "    print('Calculating medians...')\n",
    "    ROI_ice_velocity_mean = ROI_ice_velocity['S'].median(dim=['y', 'x'], skipna=True) # calculate median velocity\n",
    "    U_ROI_ice_velocity_mean = U_ROI_ice_velocity['U'].median(dim=['y', 'x'], skipna=True) # calculate median velocity\n",
    "    V_ROI_ice_velocity_mean = V_ROI_ice_velocity['V'].median(dim=['y', 'x'], skipna=True) # calculate median velocity\n",
    "\n",
    "    time_ROI = pd.to_datetime(U_ROI_ice_velocity_mean['time'].values)\n",
    "\n",
    "    velocity = ROI_ice_velocity_mean.values\n",
    "\n",
    "    print('Calculating errors...')\n",
    "    error = (U_STD*(((U_ROI_ice_velocity_mean**2)**0.5)/ROI_ice_velocity_mean)) + (V_STD*(((V_ROI_ice_velocity_mean**2)**0.5)/ROI_ice_velocity_mean))\n",
    "    error = error.values\n",
    "\n",
    "    # delete timesteps with NaN or -9999 velocity \n",
    "    error = np.delete(error, np.argwhere(np.isnan(ROI_ice_velocity_mean.values)))\n",
    "    error = np.delete(error, np.where(ROI_ice_velocity_mean.values < -10)) # less than 10 to remove -999 or -9999 values in mean, could change to .any()\n",
    "\n",
    "    velocity = np.delete(ROI_ice_velocity_mean.values, np.argwhere(np.isnan(ROI_ice_velocity_mean.values)))\n",
    "    velocity = np.delete(velocity, np.where(ROI_ice_velocity_mean.values < -10))\n",
    "\n",
    "    time_ROI = np.delete(time_ROI, np.argwhere(np.isnan(ROI_ice_velocity_mean.values)))\n",
    "    time_ROI = np.delete(time_ROI, np.where(ROI_ice_velocity_mean.values < -10))\n",
    "\n",
    "    return velocity, error, time_ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# LOAD IN METADATA FOR STANDARD DEVIATION #########################\n",
    "col_names = ['start_year', 'start_month', 'start_day', 'end_year', 'end_month', 'end_day', 'min_off-ice_vel', 'max_off-ice_vel', 'mean_off-ice_vel', 'median_off-ice_vel', 'std_off-ice_vel']\n",
    "\n",
    "u_std_file = pd.read_csv(external_data_dir + '\\AS_S1_velocities\\U\\U_metadata_250m_20160103_20230529.txt', header=None, delimiter=',', usecols=[0,1,2,6,7,8,12,13,14,15,16], names=col_names) #skips columns with 0\n",
    "u_std_file['start_date'] = pd.to_datetime(dict(year=u_std_file.start_year, month=u_std_file.start_month, day=u_std_file.start_day))\n",
    "u_std_file['end_date'] = pd.to_datetime(dict(year=u_std_file.end_year, month=u_std_file.end_month, day=u_std_file.end_day))\n",
    "u_std_file = u_std_file.drop(columns=['start_year', 'start_month', 'start_day', 'end_year', 'end_month', 'end_day'])\n",
    "u_std_file['time_between'] = u_std_file['start_date'] + (u_std_file['end_date'] - u_std_file['start_date'])/2 # calculate time between\n",
    "print(u_std_file)\n",
    "\n",
    "v_std_file = pd.read_csv(external_data_dir + '\\AS_S1_velocities\\V\\V_metadata_250m_20160103_20230529.txt', header=None, delimiter=',', usecols=[0,1,2,6,7,8,12,13,14,15,16], names=col_names) #skips columns with 0\n",
    "v_std_file['start_date'] = pd.to_datetime(dict(year=v_std_file.start_year, month=v_std_file.start_month, day=v_std_file.start_day))\n",
    "v_std_file['end_date'] = pd.to_datetime(dict(year=v_std_file.end_year, month=v_std_file.end_month, day=v_std_file.end_day))\n",
    "v_std_file = v_std_file.drop(columns=['start_year', 'start_month', 'start_day', 'end_year', 'end_month', 'end_day'])\n",
    "v_std_file['time_between'] = v_std_file['start_date'] + (v_std_file['end_date'] - v_std_file['start_date'])/2 # calculate time between\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_tiff_list = glob.glob(external_data_dir + '\\AS_S1_velocities\\S\\*timefiltered.tif')\n",
    "U_tiff_list = glob.glob(external_data_dir + '\\AS_S1_velocities\\U\\*timefiltered.tif')\n",
    "V_tiff_list = glob.glob(external_data_dir + '\\AS_S1_velocities\\V\\*timefiltered.tif')\n",
    "\n",
    "\n",
    "#Create variable used for time axis\n",
    "extracted_time, repeat_time = retrieve_timestamp(S_tiff_list)\n",
    "time_var = xr.Variable('time', extracted_time)\n",
    "\n",
    "geotiffs_ds = xr.concat([xr.open_rasterio(i).chunk('auto') for i in S_tiff_list], dim=time_var)\n",
    "geotiffs_ds = geotiffs_ds.to_dataset('band') # convert to dataset\n",
    "ice_velocity = geotiffs_ds.rename({1: 'S'}) # rename band to speed\n",
    "\n",
    "ice_velocity['repeat_time'] = (('time'), repeat_time)\n",
    "ice_velocity['repeat_time'].attrs['long_name'] = 'Sentinel1_repeat_time'\n",
    "\n",
    "########### U AND V  #################\n",
    "geotiffs_ds_U = xr.concat([xr.open_rasterio(i).chunk('auto') for i in U_tiff_list], dim=time_var)\n",
    "geotiffs_ds_U = geotiffs_ds_U.to_dataset('band') # convert to dataset\n",
    "U_velocity = geotiffs_ds_U.rename({1: 'U'}) # rename band to speed\n",
    "\n",
    "geotiffs_ds_V = xr.concat([xr.open_rasterio(i).chunk('auto') for i in V_tiff_list], dim=time_var)\n",
    "geotiffs_ds_V = geotiffs_ds_V.to_dataset('band') # convert to dataset\n",
    "V_velocity = geotiffs_ds_V.rename({1: 'V'}) # rename band to speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_data_dir = # data directory in github repo\n",
    "external_data_dir = # data directory on external hard drive\n",
    "\n",
    "ROI_list = ['IS_ROI-1', 'RUSSELL_ROI-2', 'KAN_L_ROI', 'NORTH_NUNATAK_ROI-3', 'SOUTH_NUNATAK_ROI-4', 'UNNAMED_SOUTH_ROI-5']\n",
    "for ROI in ROI_list:\n",
    "    print('\\n=========================================')\n",
    "    print('Extracting velocity for:', ROI)\n",
    "    print('=========================================\\n')\n",
    "\n",
    "    velocity, error, time_ROI = extract_ROI_velocity(ice_velocity, U_velocity, V_velocity, ROI, u_std_file['std_off-ice_vel'], v_std_file['std_off-ice_vel'])\n",
    "\n",
    "    ROI_array = np.array([velocity, error])\n",
    "    ROI_array = np.transpose(ROI_array)\n",
    "    ROI_df = pd.DataFrame(ROI_array, columns=['S', 'STD'])\n",
    "    ROI_df.index = time_ROI\n",
    "    ROI_df.to_csv(internal_data_dir + '\\velocity\\%s_timeseries.csv' % ROI)\n",
    "    print('\\nCreated CSV for', ROI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JIF_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
